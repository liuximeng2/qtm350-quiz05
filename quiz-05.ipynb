{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz 05 - Parallel Computing, Reproducibility, and Containers\n",
    "\n",
    "### Instructions\n",
    "\n",
    "This quiz is based on the material covered in lectures 21 to 24. You may use\n",
    "any resources available to you, including the lecture notes and the internet.\n",
    "\n",
    "All the data required for this quiz can be found in the `data` folder within this repository. If you need to recreate the datasets, you can do so by running the Python script included in the `script-data-generation` folder. Please make sure that the following Python packages are installed:\n",
    "\n",
    "```bash\n",
    "pip install numpy pandas pyarrow dask dask-sql joblib SQLAlchemy\n",
    "```\n",
    "\n",
    "This notebook contains the questions you need to answer.\n",
    "If possible, please submit your answers as an `.html` file on Canvas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 01 - Parallelising a Function with Joblib\n",
    "\n",
    "Use `joblib` to parallelise the computation of squaring numbers in a large array. Import the required packages and write code that uses four cores to parallelise the computation.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "def square(x):\n",
    "    return x ** 2\n",
    "\n",
    "numbers = np.arange(1000000)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 02 - Using Dask Arrays for Large Data\n",
    "\n",
    "Using Dask's `array` module, create a Dask array of random numbers with 10,000 rows and 10,000 columns. The array should be divided into chunks of 1,000 rows by 1,000 columns to enable efficient parallel computation. Populate the array with random numbers drawn from a normal distribution, where the mean is 0 and the standard deviation is 1. After creating the array, compute the mean, standard deviation, maximum, and minimum of the array using Dask's parallel computation capabilities. Use the `.compute()` method to execute the computations and print the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 03 - Dask DataFrame Operations with Parquet Files\n",
    "\n",
    "The `data` folder containts datasets for four countries—Brazil, India, UK, and USA—covering the years 1945 to 2023. Each country's data is stored in a separate Parquet file named after the country (`Brazil.parquet`, `India.parquet`, `UK.parquet`, `USA.parquet`). Each file contains the following columns:\n",
    "\n",
    "- `country` (string): The name of the country.\n",
    "- `year` (integer): The year of the record.\n",
    "- `gdp_per_capita` (float): The GDP per capita for that country and year.\n",
    "- `population` (integer): The population for that country and year.\n",
    "\n",
    "Using Dask's `dataframe` module, read _only the `country` and the `gdp_per_capita` columns_ from the Parquet files into a Dask DataFrame. Then, compute the mean and standard deviation of the GDP per capita for each country using Dask's parallel computation capabilities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 04 - Dask and SQL Queries\n",
    "\n",
    "Load the `data.csv` file into a Dask DataFrame and use the `dask_sql` package to perform a SQL query that selects the `country` and `gdp_per_capita` columns and filters the rows where `gdp_per_capita` is greater than 20000 in 2014. Display the results. Do not forget to register the Dask DataFrame as a SQL table with the `create_table` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 05 - Parallelising a Function with Dask Delayed\n",
    "\n",
    "Suppose we need to compute the sum of squares of numbers for large ranges. The function below calculates the sum of squares from `0` up to `n-1`. Modify the given `sum_of_squares` function to use Dask's `@delayed` decorator and compute the sum of squares for each number in the numbers list in parallel. Measure and print the total execution time for the parallel computation, and print the results for each input number (as indicated in the code).\n",
    "\n",
    "```python\n",
    "import time\n",
    "\n",
    "def sum_of_squares(n):\n",
    "    \"\"\"Compute the sum of squares from 0 to n-1.\"\"\"\n",
    "    return sum(i * i for i in range(n))\n",
    "\n",
    "numbers = [100_000_000, 200_000_000, 300_000_000, 400_000_000]\n",
    "\n",
    "# Measure the start time\n",
    "start_time = time.time()\n",
    "\n",
    "# Perform the computations serially\n",
    "results_serial = []\n",
    "for n in numbers:\n",
    "    result = sum_of_squares(n)\n",
    "    results_serial.append(result)\n",
    "    print(f\"Sum of squares up to {n}: {result}\")\n",
    "\n",
    "# Measure the end time\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate and print the total execution time\n",
    "serial_execution_time = end_time - start_time\n",
    "print(f\"\\nSerial execution time: {serial_execution_time:.2f} seconds\")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 06 - Using `pip` and `requirements.txt` for Dependency Management\n",
    "\n",
    "Explain how you can use `pip` to manage dependencies in a Python project. Describe the process of generating a `requirements.txt` file from your current environment and how to use this file to install the same packages in another environment or on a different machine. Please comment your code to explain each step."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Please write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 07 - Creating and Sharing a Conda Environment\n",
    "\n",
    "Describe the steps to create a new Conda virtual environment named `qtm350` with Python 3.12 and install the packages `numpy`, `pandas`, and `matplotlib`. Explain how to export this environment to an `environment.yml` file and how someone else can recreate the same environment on their machine using this file. Please comment your code to explain each step. There is no need to run the code for this question, but you can do so if you wish."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Please write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 08 - Writing a Simple Dockerfile\n",
    "\n",
    "Write a simple `Dockerfile` that creates a Docker image for a Python application. The application consists of a single Python script named `app.py` that prints \"Hello, World!\" when executed. The `Dockerfile` should use the official Python image as the base image and copy the `app.py` script into the image. When the container is run, it should execute the `app.py` script and print \"Hello, World!\"."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Please write your code here (no need to run it)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 09 - Writing a Dockerfile to Install Software on a Base Image\n",
    "\n",
    "Create a Dockerfile that starts from an Ubuntu 24.04 base image and installs the following software:\n",
    "\n",
    "- Git version 2.43.0-1ubuntu7.1\n",
    "- SQLite version 3.45.1-1ubuntu2\n",
    "\n",
    "Ensure that you specify the exact versions of the packages. Include commands to clean up the package manager cache after installation to reduce the image size."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Please write your code here (no need to run it)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### Question 10 - Writing a Dockerfile to Install Python and Packages on Ubuntu\n",
    "\n",
    "Write a `Dockerfile` that starts from an Ubuntu 24.04 base image, installs Python 3.12 and `pip`, and then uses `pip` to install specific versions of `numpy` (1.26.4), `pandas` (2.2.2), and `matplotlib` (3.9.2). Ensure you include commands to clean up the package manager cache after installation to reduce the image size. Set up a working directory named `app/` and configure the container to start an interactive Python shell `python3` by default."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Please write your code here (no need to run it)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
